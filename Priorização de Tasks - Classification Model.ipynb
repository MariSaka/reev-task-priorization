{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando um dataframe de flow_recipients com balanceamento de classe\n",
    "\n",
    "**Targets**  \n",
    "1 - connected + client  \n",
    "2 - connected  \n",
    "3 - converting  \n",
    "4 - lost + finished e parou no meio do fluxo  \n",
    "5 - lost + finished e passou por todo o fluxo\n",
    "\n",
    "\n",
    "**Baseline Acurácia**  \n",
    "rf: 0.49  \n",
    "xgb: 0.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando Tasks Normais\n",
    "**Features:**  \n",
    " c_email\t\n",
    " c_contact_id\t\n",
    " t_company_id\t\n",
    " t_done_at\t\n",
    " t_service\t\n",
    " fr_status\t\n",
    " fr_contact_id\t\n",
    " fr_flow_id\t\n",
    " fa_steps\t\n",
    " fr_last_flow_action_taken\t\n",
    " target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import pandas as pd\n",
    "\n",
    "def get_dataframe(limit = 50000):\n",
    "\n",
    "    dfs = []\n",
    "    \n",
    "    query = '''\n",
    "            select\n",
    "                c.email as c_email,\n",
    "                c.id as c_contact_id,\n",
    "                t.company_id as t_company_id,\n",
    "                t.done_at as t_done_at,\n",
    "                t.service as t_service,\n",
    "                fr.status as fr_status,\n",
    "                fr.contact_id as fr_contact_id,\n",
    "                fr.flow_id as fr_flow_id,\n",
    "                count(fa.flow_id) as fa_steps,\n",
    "                (fr.last_flow_action_taken + 1) as fr_last_flow_action_taken,\n",
    "                (target)\n",
    "            from \n",
    "                flow_recipients fr join tasks t on fr.contact_id = t.referenceable_id \n",
    "                join flow_actions fa on fr.flow_id = fa.flow_id\n",
    "                join contacts c on fr.contact_id = c.id \n",
    "            (where)\n",
    "            group by\n",
    "                c.id,\n",
    "                t.company_id,\n",
    "                fr.contact_id,\n",
    "                c.email,\n",
    "                fa.flow_id, \n",
    "                t.type,\n",
    "                t.done_at, \n",
    "                t.service, \n",
    "                t.type, \n",
    "                fr.status, \n",
    "                fr.flow_id, \n",
    "                fr.last_flow_action_taken\n",
    "            (having)\n",
    "            limit\n",
    "                {}\n",
    "        '''.format(limit)\n",
    "    \n",
    "    # connected + client\n",
    "    target = '''\n",
    "                'connected_client' as target\n",
    "            '''\n",
    "    \n",
    "    where = '''\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and (fr.status = 'finished' or fr.status = 'connected')\n",
    "                and fr.contact_final_stage = 'client' \n",
    "                and t.referenceable_type = 'Contact' \n",
    "                and t.done_at is not null\n",
    "                and t.type = 'ManualTask'\n",
    "                and t.created_by_type = 'FlowAction'\n",
    "            '''\n",
    "    \n",
    "    query_to_execute = query.replace('(target)', target).replace('(where)', where).replace('(having)', '')\n",
    "    df = pd.read_sql(query_to_execute, os.environ['REEVAPI_URL'])\n",
    "    dfs.append(df)\n",
    "    \n",
    "    # connected\n",
    "    target = '''\n",
    "                'connected' as target\n",
    "            '''\n",
    "    \n",
    "    where = '''\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and fr.status = 'connected' \n",
    "                and fr.contact_final_stage <> 'client' \n",
    "                and t.referenceable_type = 'Contact' \n",
    "                and t.done_at is not null\n",
    "                and t.type = 'ManualTask'\n",
    "                and t.created_by_type = 'FlowAction'\n",
    "           '''\n",
    "    \n",
    "    query_to_execute = query.replace('(target)', target).replace('(where)', where).replace('(having)', '')\n",
    "    df = pd.read_sql(query_to_execute, os.environ['REEVAPI_URL'])\n",
    "    dfs.append(df)\n",
    "\n",
    "    # converting\n",
    "    target = '''\n",
    "                'converting' as target\n",
    "            '''\n",
    "    \n",
    "    where = '''\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and fr.status = 'converting'  \n",
    "                and t.referenceable_type = 'Contact' \n",
    "                and t.done_at is not null\n",
    "                and t.type = 'ManualTask'\n",
    "                and t.created_by_type = 'FlowAction'\n",
    "        '''\n",
    "    \n",
    "    query_to_execute = query.replace('(target)', target).replace('(where)', where).replace('(having)', '')\n",
    "    df = pd.read_sql(query_to_execute, os.environ['REEVAPI_URL'])\n",
    "    dfs.append(df)\n",
    "\n",
    "    # lost + finished e parou no meio do fluxo\n",
    "    target = '''\n",
    "                'lost' as target\n",
    "            '''\n",
    "    \n",
    "    where = '''\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and fr.status = 'finished' \n",
    "                and fr.contact_final_stage = 'lost' \n",
    "                and t.referenceable_type = 'Contact' \n",
    "                and t.done_at is not null\n",
    "                and t.type = 'ManualTask'\n",
    "                and t.created_by_type = 'FlowAction'\n",
    "            '''\n",
    "\n",
    "    having = '''\n",
    "                having count(fa.flow_id) <> (fr.last_flow_action_taken + 1)\n",
    "            '''\n",
    "    \n",
    "    query_to_execute = query.replace('(target)', target).replace('(where)', where).replace('(having)', having)\n",
    "    df = pd.read_sql(query_to_execute, os.environ['REEVAPI_URL'])\n",
    "    dfs.append(df)\n",
    "\n",
    "    # lost + finished e passou por todo o fluxo\n",
    "    target = '''\n",
    "                'lost_whole_flow' as target\n",
    "            '''\n",
    "    \n",
    "    where = '''\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and fr.status = 'finished' \n",
    "                and fr.contact_final_stage = 'lost' \n",
    "                and t.referenceable_type = 'Contact' \n",
    "                and t.done_at is not null\n",
    "                and t.type = 'ManualTask'\n",
    "                and t.created_by_type = 'FlowAction'\n",
    "            '''\n",
    "\n",
    "    having = '''\n",
    "                having count(fa.flow_id) = (fr.last_flow_action_taken + 1)\n",
    "            '''\n",
    "            \n",
    "    query_to_execute = query.replace('(target)', target).replace('(where)', where).replace('(having)', having)    \n",
    "    df = pd.read_sql(query_to_execute, os.environ['REEVAPI_URL'])\n",
    "    dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks = get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks['ozzy'] = 0\n",
    "df_tasks = df_tasks.reindex(sorted(df_tasks.columns), axis=1)\n",
    "df_tasks.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando Ozzy Tasks Tipo 1 (fluxo associado)   \n",
    "**Features:**  \n",
    " c_email\t\n",
    " c_contact_id\t\n",
    " t_company_id\t\n",
    " t_done_at\t\n",
    " t_service\t\n",
    " fr_status\t\n",
    " fr_contact_id\t\n",
    " fr_flow_id\t\n",
    " fa_steps\t\n",
    " fr_last_flow_action_taken\t\n",
    " target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera toda as ozzy tasks já executadas (ManualTask) e seus eventos associados\n",
    "import os\n",
    "import utils\n",
    "import pandas as pd\n",
    "\n",
    "query = '''select \n",
    "            e.eventable_id as contact_id,\n",
    "            e.task_id,\n",
    "            e.eventable_type,\n",
    "            t.done_at\n",
    "           from events e join tasks t on e.task_id = t.id \n",
    "           where (t.metadata->'ozzy')::boolean is True\n",
    "            and t.type = 'ManualTask'\n",
    "           '''\n",
    "df_tmp = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "df_tmp['done_at'] = pd.to_datetime(df_tmp['done_at'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um conjunto apenas com os ids de contatos que tiveram uma ozzy task associada\n",
    "contacts = set()\n",
    "\n",
    "for i in range(0, df_tmp.shape[0]):\n",
    "    contacts.add(int(df_tmp.contact_id[i]))\n",
    "\n",
    "# Criando uma lista de contatos e datas de execução da ozzy task associada\n",
    "contacts_dates_tasks = []\n",
    "\n",
    "for i in range(0, df_tmp.shape[0]):\n",
    "    contacts_dates_tasks.append((int(df_tmp.contact_id[i]), df_tmp.done_at[i], df_tmp.task_id[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        select \n",
    "            c.email as c_email,\n",
    "            c.id as c_contact_id,\n",
    "            fr.status as fr_status,\n",
    "            fr.contact_id as fr_contact_id,\n",
    "            fr.created_at as fr_created_at,\n",
    "            fr.status_updated_at as fr_status_updated_at,\n",
    "            fr.flow_id as fr_flow_id,\n",
    "            fr.contact_final_stage as fr_contact_final_stage,\n",
    "            count(fa.flow_id) as fa_steps,\n",
    "            (fr.last_flow_action_taken + 1) as fr_last_flow_action_taken\n",
    "        from \n",
    "            flow_recipients fr\n",
    "        join \n",
    "            flow_actions fa on fr.flow_id = fa.flow_id\n",
    "        join\n",
    "            contacts c on fr.contact_id = c.id\n",
    "        where\n",
    "            fr.contact_id in (**{}**)\n",
    "        group by\n",
    "            c.email,\n",
    "            c.id,\n",
    "            fr.flow_id,\n",
    "            fr.contact_id,\n",
    "            fr.status,  \n",
    "            fr.created_at,\n",
    "            fr.status_updated_at,\n",
    "            fr.contact_final_stage,\n",
    "            fr.last_flow_action_taken\n",
    "           '''.format(str(contacts))\n",
    "\n",
    "query = query.replace('**{', '').replace('}**', '')\n",
    "df_tmp = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "df_tmp['fr_created_at'] = pd.to_datetime(df_tmp['fr_created_at'], errors='coerce')\n",
    "df_tmp['fr_status_updated_at'] = pd.to_datetime(df_tmp['fr_status_updated_at'], errors='coerce')\n",
    "\n",
    "# removendo NaNs e tratando valores float\n",
    "df_tmp.fr_last_flow_action_taken = df_tmp.fr_last_flow_action_taken.fillna(0)\n",
    "df_tmp.fr_last_flow_action_taken = df_tmp.fr_last_flow_action_taken.astype(int)\n",
    "\n",
    "df_tmp['t_id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrando pela data/hora de execução da ozzy task e adicionando id da task\n",
    "import datetime\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in contacts_dates_tasks:\n",
    "    temp = df_tmp[(df_tmp['c_contact_id'] == i[0]) & (df_tmp['fr_created_at'] < i[1]) & (df_tmp['fr_status_updated_at'] > i[1])]\n",
    "    temp['t_id'] = i[2]\n",
    "    dfs.append(temp)\n",
    "    \n",
    "df_ozzies = pd.concat(dfs)\n",
    "df_ozzies = df_ozzies.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ozzies['ozzy'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando a feature Target\n",
    "def get_target(fr_status, fr_contact_final_stage, fa_steps, fr_last_flow_action_taken):\n",
    "    # connected + client\n",
    "    if fr_status == 'connected' and fr_contact_final_stage == 'client':\n",
    "        return 'connected_client'\n",
    "\n",
    "    # connected\n",
    "    if fr_status == 'connected' and fr_contact_final_stage != 'client':\n",
    "        return  'connected'\n",
    "        \n",
    "    # converting\n",
    "    if fr_status == 'converting':\n",
    "        return 'converting'\n",
    "        \n",
    "    # lost + finished e parou no meio do fluxo\n",
    "    if fr_contact_final_stage == 'lost' and fa_steps != fr_last_flow_action_taken:\n",
    "        return 'lost'\n",
    "\n",
    "    # lost + finished e passou por todo o fluxo\n",
    "    if fr_contact_final_stage == 'lost' and fa_steps == fr_last_flow_action_taken:\n",
    "        return 'lost_whole_flow'\n",
    "\n",
    "    \n",
    "df_ozzies['target']= df_ozzies.apply(lambda x: get_target(x['fr_status'],x['fr_contact_final_stage'],x['fa_steps'],x['fr_last_flow_action_taken']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ozzies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importante notar que há muitos flow_recipients que não se enquadram em nenhum dos targets\n",
    "# deixando apenas tasks que foram ser associadas a um target\n",
    "df_ozzies = df_ozzies[df_ozzies.target.notnull()]\n",
    "df_ozzies = df_ozzies.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_task_ids = set()\n",
    "\n",
    "for i in range(0, df_ozzies.shape[0]):\n",
    "    set_task_ids.add(int(df_ozzies.t_id[i]))\n",
    "\n",
    "def get_ozzy_extra_features(df):\n",
    "    query = '''\n",
    "        select \n",
    "            t.id as t_id,\n",
    "            t.company_id as t_company_id,\n",
    "            t.done_at as t_done_at,\n",
    "            t.service as t_service\n",
    "        from \n",
    "            tasks t\n",
    "        where\n",
    "            t.id in (**{}**)\n",
    "           '''.format(str(set_task_ids))\n",
    "\n",
    "    query = query.replace('**{', '').replace('}**', '')\n",
    "    df_tmp = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "    \n",
    "    df = pd.merge(df, df_tmp, on='t_id')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_ozzies = get_ozzy_extra_features(df_ozzies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ozzies.drop(['fr_contact_final_stage', 'fr_created_at', 'fr_status_updated_at', 't_id'], axis=1, inplace=True)\n",
    "df_ozzies = df_ozzies.reindex(sorted(df_ozzies.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando Ozzy Tasks Tipo 2 (sem fluxo associado)   \n",
    "**Features:**  \n",
    " c_email\t\n",
    " c_contact_id\t\n",
    " t_company_id\t\n",
    " t_done_at\t\n",
    " t_service\t\n",
    " fr_status\t\n",
    " fr_contact_id\t\n",
    " fr_flow_id\t\n",
    " fa_steps\t\n",
    " fr_last_flow_action_taken\t\n",
    " target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera toda as ozzy tasks já executadas (ManualTask) e seus eventos associados\n",
    "import os\n",
    "import utils\n",
    "import pandas as pd\n",
    "\n",
    "query = '''select \n",
    "            e.eventable_id as contact_id,\n",
    "            e.task_id,\n",
    "            e.eventable_type,\n",
    "            t.done_at\n",
    "           from events e join tasks t on e.task_id = t.id \n",
    "           where (t.metadata->'ozzy')::boolean is True\n",
    "            and t.type = 'ManualTask'\n",
    "           '''\n",
    "#df_tmp = pd.read_sql(query, os.environ['REEVAPI_URL_FORK'])\n",
    "#df_tmp['done_at'] = pd.to_datetime(df_tmp['done_at'], errors='coerce')\n",
    "\n",
    "# Criando um conjunto apenas com os ids de contatos que tiveram uma ozzy task associada\n",
    "#contacts = set()\n",
    "\n",
    "#for i in range(0, df_tmp.shape[0]):\n",
    "#    contacts.add(int(df_tmp.contact_id[i]))\n",
    "\n",
    "# Criando uma lista de contatos e datas de execução da ozzy task associada\n",
    "#contacts_dates_tasks = []\n",
    "\n",
    "#for i in range(0, df_tmp.shape[0]):\n",
    "#    contacts_dates_tasks.append((int(df_tmp.contact_id[i]), df_tmp.done_at[i], df_tmp.task_id[i]))\n",
    "\n",
    "    \n",
    "query = '''\n",
    "        select \n",
    "            c.email as c_email,\n",
    "            c.id as c_contact_id,\n",
    "            fr.status as fr_status,\n",
    "            fr.contact_id as fr_contact_id,\n",
    "            fr.created_at as fr_created_at,\n",
    "            fr.status_updated_at as fr_status_updated_at,\n",
    "            fr.flow_id as fr_flow_id,\n",
    "            fr.contact_final_stage as fr_contact_final_stage,\n",
    "            count(fa.flow_id) as fa_steps,\n",
    "            (fr.last_flow_action_taken + 1) as fr_last_flow_action_taken\n",
    "        from \n",
    "            flow_recipients fr\n",
    "        join \n",
    "            flow_actions fa on fr.flow_id = fa.flow_id\n",
    "        join\n",
    "            contacts c on fr.contact_id = c.id\n",
    "        where\n",
    "            fr.contact_id in (**{}**)\n",
    "        group by\n",
    "            c.email,\n",
    "            c.id,\n",
    "            fr.flow_id,\n",
    "            fr.contact_id,\n",
    "            fr.status,  \n",
    "            fr.created_at,\n",
    "            fr.status_updated_at,\n",
    "            fr.contact_final_stage,\n",
    "            fr.last_flow_action_taken\n",
    "           '''.format(str(contacts))\n",
    "\n",
    "#query = query.replace('**{', '').replace('}**', '')\n",
    "#df_tmp = pd.read_sql(query, os.environ['REEVAPI_URL_FORK'])\n",
    "#df_tmp['fr_created_at'] = pd.to_datetime(df_tmp['fr_created_at'], errors='coerce')\n",
    "#df_tmp['fr_status_updated_at'] = pd.to_datetime(df_tmp['fr_status_updated_at'], errors='coerce')\n",
    "\n",
    "# removendo NaNs e tratando valores float\n",
    "#df_tmp.fr_last_flow_action_taken = df_tmp.fr_last_flow_action_taken.fillna(0)\n",
    "#df_tmp.fr_last_flow_action_taken = df_tmp.fr_last_flow_action_taken.astype(int)\n",
    "\n",
    "# recupera o id de todas as tasks que não possuem um flow_recipient associado\n",
    "#set_task_ids2 = set()\n",
    "\n",
    "#for i in contacts_dates_tasks:\n",
    "#    temp = df_tmp[(df_tmp['c_contact_id'] == i[0]) & (df_tmp['fr_created_at'] < i[1]) & (df_tmp['fr_status_updated_at'] > i[1])]\n",
    "    \n",
    "#    if temp.shape[0] == 0:\n",
    "#        set_task_ids2.add(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        select \n",
    "            c.email as c_email,\n",
    "            c.id as c_contact_id,\n",
    "            fr.status as fr_status,\n",
    "            fr.contact_id as fr_contact_id,\n",
    "            fr.created_at as fr_created_at,\n",
    "            fr.status_updated_at as fr_status_updated_at,\n",
    "            fr.flow_id as fr_flow_id,\n",
    "            fr.contact_final_stage as fr_contact_final_stage,\n",
    "            count(fa.flow_id) as fa_steps,\n",
    "            (fr.last_flow_action_taken + 1) as fr_last_flow_action_taken\n",
    "        from \n",
    "            flow_recipients fr\n",
    "        join \n",
    "            flow_actions fa on fr.flow_id = fa.flow_id\n",
    "        join\n",
    "            contacts c on fr.contact_id = c.id\n",
    "        where\n",
    "            fr.contact_id in (**{}**)\n",
    "        group by\n",
    "            c.email,\n",
    "            c.id,\n",
    "            fr.flow_id,\n",
    "            fr.contact_id,\n",
    "            fr.status,  \n",
    "            fr.created_at,\n",
    "            fr.status_updated_at,\n",
    "            fr.contact_final_stage,\n",
    "            fr.last_flow_action_taken\n",
    "           '''.format(str(contacts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando os dataframes em um único df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_tasks, df_ozzies])\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando Features Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tx_conversion(data):\n",
    "    def extract_value(x):\n",
    "        return x['by_stage']['conversion']['value']\n",
    "    \n",
    "    # query que levanta os status de flows\n",
    "    query = '''\n",
    "            select\n",
    "                id as fr_flow_id, recipients_statistics\n",
    "            from \n",
    "                flows\n",
    "        '''\n",
    "    \n",
    "    df2 = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "    \n",
    "    df = pd.merge(data, df2, on='fr_flow_id')\n",
    "    \n",
    "    df['tx_conversion'] = df.apply(lambda x: extract_value(x['recipients_statistics']),axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = tx_conversion(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previously_contact_any_company(data):\n",
    "    query = '''\n",
    "            select \n",
    "                email \n",
    "            from \n",
    "                contacts c join flow_recipients fr on fr.contact_id = c.id\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and (fr.status = 'connected' or fr.status = 'converting')\n",
    "            '''\n",
    "    df2 = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "    \n",
    "    emails = dict()\n",
    "\n",
    "    for i in range(0, df2.shape[0]):\n",
    "        if df2.email[i] not in emails: emails[df2.email[i]] = 0\n",
    "        emails[df2.email[i]] += 1\n",
    "        \n",
    "    def contacted_before(x):\n",
    "        if x in emails:\n",
    "            return emails[x]\n",
    "        return 0\n",
    "    \n",
    "    data['previously_contact_any_company'] = data.apply(lambda x: contacted_before(x['c_email']),axis=1)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "df = previously_contact_any_company(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previously_contact_my_company(data):\n",
    "    query = '''\n",
    "            select \n",
    "                email, company_id \n",
    "            from \n",
    "                contacts c join flow_recipients fr on fr.contact_id = c.id\n",
    "            where\n",
    "                fr.last_flow_action_taken is not null\n",
    "                and (fr.status = 'connected' or fr.status = 'converting')\n",
    "            '''\n",
    "    df2 = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "    \n",
    "    opportunities = dict()\n",
    "\n",
    "    for i in range(0, df2.shape[0]):\n",
    "        opportunity = df2.email[i] + '_' + str(df2.company_id[i])\n",
    "        \n",
    "        if opportunity not in opportunities: opportunities[opportunity] = 0\n",
    "        opportunities[opportunity] += 1\n",
    "        \n",
    "    def contacted_before(email, company_id):\n",
    "        opportunity = email + '_' + str(company_id)\n",
    "        if opportunity in opportunities:\n",
    "            return opportunities[opportunity]\n",
    "        return 0\n",
    "    \n",
    "    data['previously_contact_my_company'] = data.apply(lambda x: contacted_before(x['c_email'], x['t_company_id']),axis=1)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "df = previously_contact_my_company(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inbound(data):\n",
    "    query = '''\n",
    "            select \n",
    "                 id as c_contact_id, \n",
    "                 metadata\n",
    "            from \n",
    "                contacts\n",
    "            '''\n",
    "    df2 = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "        \n",
    "    def process_inbound(x):\n",
    "        if 'rd_station' in str(x):\n",
    "            return 'rd_station'\n",
    "        \n",
    "        if 'sharp_spring' in str(x):\n",
    "            return 'sharp_spring'\n",
    "        \n",
    "        if 'hubspot' in str(x):\n",
    "            return 'hubspot'\n",
    "        return 'not_inbound'\n",
    "        \n",
    "    \n",
    "    df2['is_inbound'] = df2.apply(lambda x: process_inbound(x['metadata']),axis=1)\n",
    "    \n",
    "    df2 = df2.drop(['metadata'], axis=1)\n",
    "    \n",
    "    df = pd.merge(data, df2, on='c_contact_id')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = is_inbound(df)\n",
    "\n",
    "def is_inbound2(data):\n",
    "    query = '''\n",
    "            select \n",
    "                 id as c_contact_id,\n",
    "                 metadata\n",
    "            from \n",
    "                contacts\n",
    "            '''\n",
    "    df2 = pd.read_sql(query, os.environ['REEVAPI_URL'])\n",
    "        \n",
    "    def process_inbound(x):\n",
    "        if 'rd_station' in str(x):\n",
    "            return True\n",
    "        \n",
    "        if 'sharp_spring' in str(x):\n",
    "            return True\n",
    "        \n",
    "        if 'hubspot' in str(x):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    df2['is_inbound2'] = df2.apply(lambda x: process_inbound(x['metadata']),axis=1)\n",
    "    \n",
    "    df2 = df2.drop(['metadata'], axis=1)\n",
    "    \n",
    "    df = pd.merge(data, df2, on='c_contact_id')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = is_inbound2(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df = df.drop(['c_email', 'c_contact_id', 't_company_id', 'fr_contact_id', 'fr_status', 'fr_flow_id', 't_done_at', 'recipients_statistics'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label enconder\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['t_service'] = le.fit_transform(df['t_service'])\n",
    "df['is_inbound'] = le.fit_transform(df['is_inbound'])\n",
    "df['is_inbound2'] = le.fit_transform(df['is_inbound2'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(target):\n",
    "    if target == 'connected_client':\n",
    "        return 4\n",
    "    if target == 'connected':\n",
    "        return 3\n",
    "    if target == 'converting':\n",
    "        return 2\n",
    "    if target == 'lost':\n",
    "        return 1\n",
    "    if target == 'lost_whole_flow':\n",
    "        return 0\n",
    "    \n",
    "df['target'] = df.apply(lambda x: transform_target(x['target']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva o status atual do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva o dataframe\n",
    "import pickle\n",
    "\n",
    "file = open('df_tasks_priorization_classfication_model', 'wb')\n",
    "pickle.dump(df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega o dataframe\n",
    "import pickle\n",
    "file = open('df_tasks_priorization_classfication_model', 'rb')\n",
    "df = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlação maior de 20% com o target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_target = abs(cor[\"target\"])\n",
    "\n",
    "relevant_features = cor_target[cor_target>0.2]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particiona em treino, teste e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, max_features=3, criterion='gini', n_jobs=8)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred_rf = rf.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test, pred_rf)))\n",
    "\n",
    "prob_rf = rf.predict_proba(X_test)\n",
    "print(\"RandomForest\\nLogloss: \" + str(log_loss(y_test, prob_rf)))\n",
    "\n",
    "scores = cross_val_score(rf, X, y, cv=5)\n",
    "print(\"Accuracy CrossValidation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.XGBClassifier(max_depth=15, n_estimators=250, learning_rate=0.05, subsample=0.7, colsample_bytree=0.5,silent=1, objective='multi:softprob')\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test, pred_xgb)))\n",
    "\n",
    "prob_xgb = xgb.predict_proba(X_test)\n",
    "print(\"XGBoost\\nLogloss: \" + str(log_loss(y_test, prob_xgb)))\n",
    "\n",
    "scores = cross_val_score(xgb, X, y, cv=5)\n",
    "print(\"Accuracy CrossValidation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um ensemble com parâmetros variados para Random Forest e XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=400, max_features=4, criterion='gini', n_jobs=8)\n",
    "rf2 = RandomForestClassifier(n_estimators=400, max_features=6, criterion='gini', n_jobs=8)\n",
    "rf3 = RandomForestClassifier(n_estimators=500, max_features=4, criterion='gini', n_jobs=8)\n",
    "rf4 = RandomForestClassifier(n_estimators=500, max_features=6, criterion='gini', n_jobs=8)\n",
    "rf5 = RandomForestClassifier(n_estimators=600, max_features=4, criterion='gini', n_jobs=8)\n",
    "rf6 = RandomForestClassifier(n_estimators=600, max_features=6, criterion='gini', n_jobs=8)\n",
    "\n",
    "rf1.fit(X_train, y_train)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf3.fit(X_train, y_train)\n",
    "rf4.fit(X_train, y_train)\n",
    "rf5.fit(X_train, y_train)\n",
    "rf6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf1 = rf1.predict(X_test)\n",
    "pred_rf2 = rf2.predict(X_test)\n",
    "pred_rf3 = rf3.predict(X_test)\n",
    "pred_rf4 = rf4.predict(X_test)\n",
    "pred_rf5 = rf5.predict(X_test)\n",
    "pred_rf6 = rf6.predict(X_test)\n",
    "\n",
    "print(\"rf1 accuracy: \" + str(accuracy_score(y_test, pred_rf1)))\n",
    "print(\"rf2 accuracy: \" + str(accuracy_score(y_test, pred_rf2)))\n",
    "print(\"rf3 accuracy: \" + str(accuracy_score(y_test, pred_rf3)))\n",
    "print(\"rf4 accuracy: \" + str(accuracy_score(y_test, pred_rf4)))\n",
    "print(\"rf5 accuracy: \" + str(accuracy_score(y_test, pred_rf5)))\n",
    "print(\"rf6 accuracy: \" + str(accuracy_score(y_test, pred_rf6)))\n",
    "\n",
    "print()\n",
    "\n",
    "prob_rf1 = rf1.predict_proba(X_test)\n",
    "prob_rf2 = rf2.predict_proba(X_test)\n",
    "prob_rf3 = rf3.predict_proba(X_test)\n",
    "prob_rf4 = rf4.predict_proba(X_test)\n",
    "prob_rf5 = rf5.predict_proba(X_test)\n",
    "prob_rf6 = rf6.predict_proba(X_test)\n",
    "\n",
    "prob_mean_rf = ((prob_rf1 + prob_rf2 + prob_rf3 + prob_rf4 + prob_rf5 + prob_rf6)/6)\n",
    "\n",
    "print(\"rf1 logloss: \" + str(log_loss(y_test, prob_rf1)))\n",
    "print(\"rf2 logloss: \" + str(log_loss(y_test, prob_rf2)))\n",
    "print(\"rf3 logloss: \" + str(log_loss(y_test, prob_rf3)))\n",
    "print(\"rf4 logloss: \" + str(log_loss(y_test, prob_rf4)))\n",
    "print(\"rf5 logloss: \" + str(log_loss(y_test, prob_rf5)))\n",
    "print(\"rf6 logloss: \" + str(log_loss(y_test, prob_rf6)))\n",
    "print(\"rf mix logloss: \" + str(log_loss(y_test, prob_mean_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb1 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.05, subsample=0.6, colsample_bytree=0.5,silent=1, objective='multi:softprob')\n",
    "xgb2 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.05, subsample=0.7, colsample_bytree=0.6,silent=1, objective='multi:softprob')\n",
    "xgb3 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.1, subsample=0.6, colsample_bytree=0.5,silent=1, objective='multi:softprob')\n",
    "xgb4 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.1, subsample=0.7, colsample_bytree=0.6,silent=1, objective='multi:softprob')\n",
    "xgb5 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.15, subsample=0.6, colsample_bytree=0.5,silent=1, objective='multi:softprob')\n",
    "xgb6 = xgb.XGBClassifier(max_depth=6, n_estimators=250, learning_rate=0.15, subsample=0.8, colsample_bytree=0.7,silent=1, objective='multi:softprob')\n",
    "\n",
    "xgb1.fit(X_train, y_train)\n",
    "xgb2.fit(X_train, y_train)\n",
    "xgb3.fit(X_train, y_train)\n",
    "xgb4.fit(X_train, y_train)\n",
    "xgb5.fit(X_train, y_train)\n",
    "xgb6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb1 = xgb1.predict(X_test)\n",
    "pred_xgb2 = xgb2.predict(X_test)\n",
    "pred_xgb3 = xgb3.predict(X_test)\n",
    "pred_xgb4 = xgb4.predict(X_test)\n",
    "pred_xgb5 = xgb5.predict(X_test)\n",
    "pred_xgb6 = xgb6.predict(X_test)\n",
    "\n",
    "print(\"xgb1 accuracy: \" + str(accuracy_score(y_test, pred_xgb1)))\n",
    "print(\"xgb2 accuracy: \" + str(accuracy_score(y_test, pred_xgb2)))\n",
    "print(\"xgb3 accuracy: \" + str(accuracy_score(y_test, pred_xgb3)))\n",
    "print(\"xgb4 accuracy: \" + str(accuracy_score(y_test, pred_xgb4)))\n",
    "print(\"xgb5 accuracy: \" + str(accuracy_score(y_test, pred_xgb5)))\n",
    "print(\"xgb6 accuracy: \" + str(accuracy_score(y_test, pred_xgb6)))\n",
    "\n",
    "print()\n",
    "\n",
    "prob_xgb1 = xgb1.predict_proba(X_test)\n",
    "prob_xgb2 = xgb2.predict_proba(X_test)\n",
    "prob_xgb3 = xgb3.predict_proba(X_test)\n",
    "prob_xgb4 = xgb4.predict_proba(X_test)\n",
    "prob_xgb5 = xgb5.predict_proba(X_test)\n",
    "prob_xgb6 = xgb6.predict_proba(X_test)\n",
    "\n",
    "prob_mean_xgb = ((prob_xgb1 + prob_xgb2 + prob_xgb3 + prob_xgb4 + prob_xgb5 + prob_xgb6)/6)\n",
    "\n",
    "print(\"xgb1 logloss: \" + str(log_loss(y_test, prob_xgb1)))\n",
    "print(\"xgb2 logloss: \" + str(log_loss(y_test, prob_xgb2)))\n",
    "print(\"xgb3 logloss: \" + str(log_loss(y_test, prob_xgb3)))\n",
    "print(\"xgb4 logloss: \" + str(log_loss(y_test, prob_xgb4)))\n",
    "print(\"xgb5 logloss: \" + str(log_loss(y_test, prob_xgb5)))\n",
    "print(\"xgb6 logloss: \" + str(log_loss(y_test, prob_xgb6)))\n",
    "print(\"xgb mix logloss: \" + str(log_loss(y_test, prob_mean_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_mean = ((prob_rf1 + prob_rf2 + prob_rf3 + prob_rf4 + prob_rf5 + prob_rf6 + prob_xgb1 + prob_xgb2 + prob_xgb3 + prob_xgb4 + prob_xgb5 + prob_xgb6)/12)\n",
    "print(\"mix logloss: \" + str(log_loss(y_test, prob_mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo um score para uma task   \n",
    "\n",
    "Como definir a melhor estratégia?\n",
    " regra de negócio? ou existe uma forma precisa de fazer isso?\n",
    " \n",
    "***\n",
    "\n",
    "**Estratégias:**   \n",
    "1. ordenando por classes e posteriormente pela probabilidade dentro da classe\n",
    "2. utilizando o somatório do peso das duas classes mais positivas contra as duas mais negativas\n",
    "3. verificar a diferença entre as classes, quando a diferença for baixa, ser otimista (considerar as classes positivas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def teste_scores():\n",
    "    # teste scores\n",
    "    _4 = []\n",
    "    _3 = []\n",
    "    _2 = []\n",
    "    _1 = []\n",
    "    _0 = []\n",
    "\n",
    "    for i in range(0, pred_rf.shape[0]):\n",
    "        score = get_score(pred_rf[i])\n",
    "\n",
    "        if (int(score)) == 0:\n",
    "            _0.append(score)\n",
    "        elif (int(score)) == 1:\n",
    "            _1.append(score)\n",
    "        elif (int(score)) == 2:\n",
    "            _2.append(score)\n",
    "        elif (int(score)) == 3:\n",
    "            _3.append(score)\n",
    "        elif (int(score)) == 4:\n",
    "            _4.append(score)\n",
    "\n",
    "    print('MIN')\n",
    "    print('classe 4', ('0.' + str(min(_4)).split('.')[1]))\n",
    "    print('classe 3', ('0.' + str(min(_3)).split('.')[1]))\n",
    "    print('classe 2', ('0.' + str(min(_2)).split('.')[1]))\n",
    "    print('classe 1', ('0.' + str(min(_1)).split('.')[1]))\n",
    "    print('classe 0', ('0.' + str(min(_0)).split('.')[1]))\n",
    "\n",
    "    print('QTD')\n",
    "    print('classe 4', len(_4))\n",
    "    print('classe 3', len(_3))\n",
    "    print('classe 2', len(_2))\n",
    "    print('classe 1', len(_1))\n",
    "    print('classe 0', len(_0))\n",
    "    \n",
    "    return ([len(_4),len(_3),len(_2),len(_1),len(_0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégia 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. ordenando por classes e posteriormente pela probabilidade dentro da classe\n",
    "import numpy as np\n",
    "\n",
    "def get_score(pred):\n",
    "    value = max(pred)\n",
    "    index = np.where(pred == max(pred))[0][0]\n",
    "\n",
    "    value = (index + value - 0.0001)\n",
    "    \n",
    "    return value\n",
    "\n",
    "strategy_1 = teste_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégia 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. utilizando o somatório do peso das duas classes mais positivas contra as duas mais negativas\n",
    "import numpy as np\n",
    "\n",
    "def get_score(pred):\n",
    "    # 4 connected_client\n",
    "    # 3 connected\n",
    "    # 2 converting\n",
    "    # 1 lost\n",
    "    # 0 lost_whole_flow\n",
    "    positive = (pred[4] + pred[3])/2\n",
    "    neutral = pred[2]\n",
    "    negative = (pred[1] + pred[0])/2\n",
    "    \n",
    "    # caso positivo (classes 4 e 3\n",
    "    if positive > neutral and positive > negative:\n",
    "        if pred[4] > pred[3]:\n",
    "            return 4 + (pred[4] - 0.0001)\n",
    "        return 3 + (pred[3] - 0.0001)\n",
    "    \n",
    "    # caso neutro (classe 2)\n",
    "    if neutral > negative:\n",
    "        return 2 + (neutral - 0.0001)\n",
    "    \n",
    "    # caso negativo (classes 1 e 0)\n",
    "    if pred[1] > pred[0]:\n",
    "        return 1 + (pred[1] - 0.0001)\n",
    "    return 0 + (pred[0] - 0.0001)\n",
    "\n",
    "strategy_2 = teste_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 30 30\n",
    "1 5 0\n",
    "2\n",
    "3 27 29\n",
    "4 10 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégia 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. verificar a diferença entre as classes, quando a diferença for baixa, ser otimista (considerar as classes positivas)\n",
    "import numpy as np\n",
    "\n",
    "def get_score(pred, threshold=0.5):\n",
    "    # 4 connected_client\n",
    "    # 3 connected\n",
    "    # 2 converting\n",
    "    # 1 lost\n",
    "    # 0 lost_whole_flow\n",
    "    threshold = 0.05\n",
    "    \n",
    "    value = max(pred)\n",
    "    index = np.where(pred == max(pred))[0][0]\n",
    "    \n",
    "    if index != 4:\n",
    "        if (value - pred[4]) < threshold:\n",
    "            return 4 + (pred[4] - 0.0001)\n",
    "    \n",
    "    if index != 3:\n",
    "        if (value - pred[3]) < threshold:\n",
    "            return 3 + (pred[3] - 0.0001)\n",
    "        \n",
    "    value = (index + value - 0.0001)\n",
    "    \n",
    "    return value\n",
    "strategy_3 = teste_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "barWidth = 0.25\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "r1 = np.arange(len(strategy_1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "plt.bar(r1, strategy_1, width=barWidth, label='Classificação Normal')\n",
    "plt.bar(r2, strategy_2, width=barWidth, label='Classes Pos X Neg')\n",
    "plt.bar(r3, strategy_3, width=barWidth, label='Otimista')\n",
    "\n",
    "plt.xlabel('Classes')\n",
    "plt.xticks([r + barWidth for r in range(len(strategy_1))], ['connected_client', 'connected', 'converting', 'lost', 'lost_whole_flow'])\n",
    "plt.ylabel('Número de tasks')\n",
    "plt.title('Comparação Estratégias')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### procurando pelo melhor threshold para a estratégia 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_score(pred, threshold=0.5):\n",
    "    # 4 connected_client\n",
    "    # 3 connected\n",
    "    # 2 converting\n",
    "    # 1 lost\n",
    "    # 0 lost_whole_flow\n",
    "    \n",
    "    value = max(pred)\n",
    "    index = np.where(pred == max(pred))[0][0]\n",
    "    \n",
    "    if index != 4:\n",
    "        if (value - pred[4]) < threshold:\n",
    "            return 4 + (pred[4] - 0.0001)\n",
    "    \n",
    "    if index != 3:\n",
    "        if (value - pred[3]) < threshold:\n",
    "            return 3 + (pred[3] - 0.0001)\n",
    "        \n",
    "    value = (index + value - 0.0001)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_thresholds(pred):\n",
    "    thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.4, 0.5, 0.7, 0.9, 1.0]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        _4 = 0\n",
    "        _3 = 0\n",
    "        _2 = 0\n",
    "        _1 = 0\n",
    "        _0 = 0\n",
    "\n",
    "        for i in range(0, pred.shape[0]):\n",
    "            score = get_score(pred[i], threshold)\n",
    "\n",
    "            if (int(score)) == 0:\n",
    "                _0 += 1\n",
    "            elif (int(score)) == 1:\n",
    "                _1 += 1\n",
    "            elif (int(score)) == 2:\n",
    "                _2 += 1\n",
    "            elif (int(score)) == 3:\n",
    "                _3 += 1\n",
    "            elif (int(score)) == 4:\n",
    "                _4 += 1\n",
    "                \n",
    "        plt.plot([_0, _1, _2, _3, _4], label=threshold)\n",
    "        \n",
    "    plt.title('Comparação Thresholds ')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Número de tasks')\n",
    "    plt.show()\n",
    "\n",
    "get_thresholds(prob_mean_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_score(pred, threshold=0.5):\n",
    "    # 4 connected_client\n",
    "    # 3 connected\n",
    "    # 2 converting\n",
    "    # 1 lost\n",
    "    # 0 lost_whole_flow\n",
    "    \n",
    "    value = max(pred)\n",
    "    index = np.where(pred == max(pred))[0][0]\n",
    "    \n",
    "    if index != 4:\n",
    "        if (value - pred[4]) < threshold:\n",
    "            return 4 + (pred[4] - 0.0001)\n",
    "    \n",
    "    if index != 3:\n",
    "        if (value - pred[3]) < threshold:\n",
    "            return 3 + (pred[3] - 0.0001)\n",
    "        \n",
    "    value = (index + value - 0.0001)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_thresholds(pred):\n",
    "    thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 1.0]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        _4 = 0\n",
    "        _3 = 0\n",
    "        _2 = 0\n",
    "        _1 = 0\n",
    "        _0 = 0\n",
    "\n",
    "        for i in range(0, pred.shape[0]):\n",
    "            score = get_score(pred[i], threshold)\n",
    "\n",
    "            if (int(score)) == 3:\n",
    "                _3 += 1\n",
    "            elif (int(score)) == 4:\n",
    "                _4 += 1\n",
    "                \n",
    "        plt.plot([_3, _4], label=threshold)\n",
    "        \n",
    "    plt.title('Comparação Thresholds ')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Classes 3 e 4')\n",
    "    plt.ylabel('Número de tasks')\n",
    "    plt.show()\n",
    "\n",
    "get_thresholds(prob_mean_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_score(pred, threshold=0.5):\n",
    "    # 4 connected_client\n",
    "    # 3 connected\n",
    "    # 2 converting\n",
    "    # 1 lost\n",
    "    # 0 lost_whole_flow\n",
    "    \n",
    "    value = max(pred)\n",
    "    index = np.where(pred == max(pred))[0][0]\n",
    "    \n",
    "    if index != 4:\n",
    "        if (value - pred[4]) < threshold:\n",
    "            return 4 + (pred[4] - 0.0001)\n",
    "    \n",
    "    if index != 3:\n",
    "        if (value - pred[3]) < threshold:\n",
    "            return 3 + (pred[3] - 0.0001)\n",
    "        \n",
    "    value = (index + value - 0.0001)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_thresholds(pred):\n",
    "    thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 1.0]\n",
    "    result = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        _4 = 0\n",
    "        _3 = 0\n",
    "        _2 = 0\n",
    "        _1 = 0\n",
    "        _0 = 0\n",
    "\n",
    "        for i in range(0, pred.shape[0]):\n",
    "            score = get_score(pred[i], threshold)\n",
    "\n",
    "            if (int(score)) == 4:\n",
    "                _4 += 1\n",
    "                \n",
    "        result.append(_4)\n",
    "        \n",
    "    plt.title('Comparação Thresholds ')\n",
    "    plt.plot(result)\n",
    "    plt.xlabel(thresholds)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "get_thresholds(prob_mean_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicas de efeitos em para outputs no notebook\n",
    "https://stackoverflow.com/questions/23271575/printing-bold-colored-etc-text-in-ipython-qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Features possíveis **   \n",
    "taxa conversão  \n",
    "taxa de abertura (só para email)  \n",
    "taxa de resposta (só para email)  \n",
    "tipos de tarefa do fluxo (tipos de tarefa)  \n",
    "informações gerais sobre o fluxo  \n",
    "contatos com interação prévia   \n",
    "origem do contato   \n",
    "quantas tarefas foram executadas anteriormente para esse contato   \n",
    "características do template associado à tarefa   \n",
    "empresa do contato   \n",
    "quantidade de contatos da mesma empresa na base   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
